@inproceedings{Dilshan2025,
abstract = {In the digital age, the recruitment landscape has changed dramatically. This study aims to optimize recruitment by automating resume screening through advanced Natural Language Processing (NLP) and Machine Learning (ML) techniques. The purpose of this research is to address the limitations of traditional Applicant Tracking Systems (ATS) by introducing a framework employing Bidirectional Encoder Representations from Transformers (BERT). A comprehensive methodology was implemented, including data preprocessing, manual dataset annotation, feature extraction via TF-IDF, and cosine similarity for ranking resumes against job descriptions. Empirical findings reveal that BERT outperforms other ML technologies including SVM, achieving a classification accuracy of 78.6%, validating its effectiveness in KSA extraction and alignment. Additionally, the system's outputs were validated by industry experts, who confirmed the accuracy and relevance of the extracted KSAs in real-world scenarios. The practical implications of this work include providing actionable insights for HR professionals, enhancing candidate-job matching, and streamlining recruitment workflows. The originality of this study lies in integrating advanced ML models for dynamic KSA classification, paving the way for more efficient recruitment practices. Future research may explore ensemble techniques and transfer learning to further refine the methodology and adapt it to diverse industries.},
author = {Dilshan, B. A Thisara and Asanka, P. P. G Dinesh},
booktitle = {2025 5th International Conference on Advanced Research in Computing (ICARC)},
doi = {10.1109/ICARC64760.2025.10963312},
isbn = {979-8-3315-3098-3},
keywords = {Knowledge-Skills-Abilities,Machine Learning,Natural Language Processing,Software Engineering},
month = {feb},
pages = {1--6},
publisher = {IEEE},
title = {{Enhancing Resume Analysis: Leveraging Natural Language Processing and Machine Learning for Automated Resume Screening Using KSA Parameters}},
url = {https://ieeexplore.ieee.org/document/10963312/},
year = {2025}
}
@inproceedings{Unlu2024,
abstract = {Software Size Measurement (SSM) plays an essential role in software project management as it enables the acquisition of software size, which is the primary input for development effort and schedule estimation. However, many small and medium-sized companies cannot perform objective SSM and Software Effort Estimation (SEE) due to the lack of resources and an expert workforce. This results in inadequate estimates and projects exceeding the planned time and budget. Therefore, organizations need to perform objective SSM and SEE using minimal resources without an expert workforce. In this research, we conducted an exploratory case study to predict the functional size of software project requirements using state-of-the-art large language models (LLMs). For this aim, we fine-tuned BERT and BERT_SE with a set of user stories and their respective functional size in COSMIC Function Points (CFP). We gathered the user stories included in different project requirement documents. In total size prediction, we achieved 72.8% accuracy with BERT and 74.4% accuracy with BERT_SE. In data movement-based size prediction, we achieved 87.5% average accuracy with BERT and 88.1% average accuracy with BERT_SE. Although we use relatively small datasets in model training, these results are promising and hold significant value as they demonstrate the practical utility of language models in SSM.},
author = {Unlu, Huseyin and Tenekeci, Samet and Ciftci, Can and Oral, Ibrahim Baran and Atalay, Tunahan and Hacaloglu, Tuna and Musaoglu, Burcu and Demirors, Onur},
booktitle = {Proceedings of the Euromicro Conference on Software Engineering and Advanced Applications, EUROMICRO-SEAA},
doi = {10.1109/SEAA64295.2024.00036},
isbn = {9798350380262},
issn = {23769521},
keywords = {BERT,COSMIC,NLP,functional size,natural language processing,software engineering,software size measurement},
month = {aug},
number = {2024},
pages = {188--193},
publisher = {IEEE},
title = {{Predicting Software Functional Size Using Natural Language Processing: An Exploratory Case Study}},
url = {https://ieeexplore.ieee.org/document/10803425/},
year = {2024}
}
@inproceedings{Altarturi2023,
abstract = {Natural Language Processing (NLP) is a major subfield of machine learning that focuses on the interaction between humans and computers through several techniques, including topic modeling, which extracts and generates topics from large amounts of unstructured data on the web. Topic modeling uncovers hidden patterns and contextual meanings in web pages, allowing for the categorization, clustering, recommendation of relevant content, detection of emerging trends, and improved information retrieval. The benefit of topic models relies on their performance to detect and generate topics; however, the evaluation of applying topic modeling on web content data remains lacking in current studies. The absence of examining such performance evaluations hinders understanding performance variability across different web pages, the development of refined models, and the ability to keep pace with emerging patterns in the dynamic online environment. This study presents a comprehensive performance evaluation of the benchmark topic models when applied to web content data. It presents a comparison of topic models using four topic coherence metrics, providing an in-depth performance analysis. The evaluation is conducted on a public dataset consisting of 2 million web pages, providing a substantial basis for assessing the strengths and limitations of these models in the context of web data. Additionally, this study examines the utilization of NLP techniques for topic generation within the context of web content and sheds light on the challenges and issues associated with generating topics from web content data, highlighting potential research prospects and the need for future work in the field of web topic modeling using natural language processing.},
author = {Altarturi, Hamza H.M. and Saadoon, Muntadher and Anuar, Nor Badrul},
booktitle = {2023 IEEE International Conference on Computing (ICOCO)},
doi = {10.1109/ICOCO59262.2023.10398059},
isbn = {979-8-3503-0268-4},
keywords = {Generate Topics,NLP,Natural Language Processing,Topic Modeling,Web Content},
month = {oct},
pages = {520--525},
publisher = {IEEE},
title = {{Examining the Role of Natural Language Processing in Generating Topics from Web Content}},
url = {https://ieeexplore.ieee.org/document/10398059/},
year = {2023}
}
@inproceedings{Shoukat2023,
abstract = {Speech emotion recognition (SER) faces challenges in cross-language scenarios due to differences in linguistic and cultural expression of emotions across languages. Recently, large multilingual foundation models pre-trained on massive corpora have achieved performance on natural language understanding tasks by learning cross-lingual representations. Their ability to understand relationships between languages without direct translation opens up possibilities for more applicable multilingual models. In this paper, we evaluate the capabilities of foundation models (Wav2Vec2, XLSR, Whisper and MMS) to bridge the gap in cross-language SER. Specifically, we analyse their performance on benchmark cross-language SER datasets involving four languages for emotion classification. Our experiments show that the foundation model outperforms CNN-LSTM baselines, establishing their superiority in cross-lingual transfer learning for emotion recognition. However, self-supervised pre-training plays a key role, and inductive biases alone are insufficient for high cross-lingual generalisability. Foundation models also demonstrate gains over baselines with limited target data and better performance on noisy data. Our findings indicate that while foundation models hold promise, pre-training remains vital for handling linguistic variations across languages for SER.},
author = {Shoukat, Moazzam and Usama, Muhammad and Ali, Hafiz Shehbaz and Latif, Siddique},
booktitle = {2023 Tenth International Conference on Social Networks Analysis, Management and Security (SNAMS)},
doi = {10.1109/SNAMS60348.2023.10375468},
isbn = {979-8-3503-1890-6},
keywords = {cross-language,foundation models,multilingual data,self-supervised learning,speech emotion recognition,transformers},
month = {nov},
pages = {1--9},
publisher = {IEEE},
title = {{Breaking Barriers: Can Multilingual Foundation Models Bridge the Gap in Cross-Language Speech Emotion Recognition?}},
url = {https://ieeexplore.ieee.org/document/10375468/},
year = {2023}
}
@article{Arkhangelskaya2023,
abstract = {Deep learning has been the mainstream technique in natural language processing (NLP) area. However, the techniques require many labeled data and are less generalizable across domains. Meta-learning is an arising field in machine learning studying approaches to learn better learning algorithms. Approaches aim at improving algorithms in various aspects, including data efficiency and generalizability. Efficacy of approaches has been shown in many NLP tasks, but there is no systematic survey of these approaches in NLP, which hinders more researchers from joining the field. Our goal with this survey paper is to offer researchers pointers to relevant meta-learning works in NLP and attract more attention from the NLP community to drive future innovation. This paper first introduces the general concepts of meta-learning and the common approaches. Then we summarize task construction settings and application of meta-learning for various NLP problems and review the development of meta-learning in NLP community.},
archivePrefix = {arXiv},
arxivId = {2205.01500},
author = {Arkhangelskaya, E. O. and Nikolenko, S. I.},
doi = {10.1007/s10958-023-06519-6},
eprint = {2205.01500},
file = {:C\:/Users/USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Lee et al. - 2022 - Meta Learning for Natural Language Processing A Survey.pdf:pdf},
issn = {1072-3374},
journal = {Journal of Mathematical Sciences},
month = {jul},
number = {4},
pages = {533--582},
publisher = {Springer},
title = {{Deep Learning for Natural Language Processing: A Survey}},
url = {https://arxiv.org/pdf/2205.01500 http://arxiv.org/abs/2205.01500 https://link.springer.com/10.1007/s10958-023-06519-6},
volume = {273},
year = {2023}
}
@article{Sarhan2020,
abstract = {Various tasks in natural language processing (NLP) suffer from lack of labelled training data, which deep neural networks are hungry for. In this paper, we relied upon features learned to generate relation triples from the open information extraction (OIE) task. First, we studied how transferable these features are from one OIE domain to another, such as from a news domain to a bio-medical domain. Second, we analyzed their transferability to a semantically related NLP task, namely, relation extraction (RE). We thereby contribute to answering the question: can OIE help us achieve adequate NLP performance without labelled data? Our results showed comparable performance when using inductive transfer learning in both experiments by relying on a very small amount of the target data, wherein promising results were achieved. When transferring to the OIE bio-medical domain, we achieved an F-measure of 78.0%, only 1% lower when compared to traditional learning. Additionally, transferring to RE using an inductive approach scored an F-measure of 67.2%, which was 3.8% lower than training and testing on the same task. Hereby, our analysis shows that OIE can act as a reliable source task.},
author = {Sarhan, Injy and Spruit, Marco},
doi = {10.3390/APP10175758},
file = {:C\:/Users/USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Sarhan, Spruit - 2020 - Can we survive without labelled data in NLP Transfer learning for open information extraction.pdf:pdf},
issn = {20763417},
journal = {Applied Sciences (Switzerland)},
keywords = {Open information extraction,Recurrent neural networks,Relation extraction,Transfer learning,Word embeddings},
month = {aug},
number = {17},
pages = {5758},
publisher = {Multidisciplinary Digital Publishing Institute},
title = {{Can we survive without labelled data in NLP? Transfer learning for open information extraction}},
url = {https://www.mdpi.com/2076-3417/10/17/5758/htm https://www.mdpi.com/2076-3417/10/17/5758},
volume = {10},
year = {2020}
}
@article{Laparra2021,
abstract = {Objectives: We survey recent work in biomedical NLP on building more adaptable or generalizable models, with a focus on work dealing with electronic health record (EHR) texts, to better understand recent trends in this area and identify opportunities for future research.},
author = {Laparra, Egoitz and Mascio, Aurelie and Velupillai, Sumithra and Miller, Timothy},
doi = {10.1055/s-0041-1726522},
file = {:C\:/Users/USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Laparra et al. - 2021 - A Review of Recent Work in Transfer Learning and Domain Adaptation for Natural Language Processing of Electronic.pdf:pdf},
issn = {0943-4747},
journal = {Yearbook of Medical Informatics},
keywords = {Natural language processing,domain adaptation,electronic health records,transfer learning},
month = {aug},
number = {01},
pages = {239--244},
pmid = {34479396},
publisher = {Thieme Medical Publishers, Inc.},
title = {{A Review of Recent Work in Transfer Learning and Domain Adaptation for Natural Language Processing of Electronic Health Records}},
url = {http://www.thieme-connect.com/products/ejournals/html/10.1055/s-0041-1726522 http://www.thieme-connect.de/DOI/DOI?10.1055/s-0041-1726522 http://www.ncbi.nlm.nih.gov/pubmed/34479396 http://www.pubmedcentral.nih.gov/articlerender.fcgi?artid=PMC8416218},
volume = {30},
year = {2021}
}
@article{Manaka2024,
abstract = {The restricted access to data in healthcare facilities due to patient privacy and confidentiality policies has led to the application of general natural language processing (NLP) techniques advancing relatively slowly in the health domain. Additionally, because clinical data is unique to various institutions and laboratories, there are not enough standards and conventions for data annotation. In places without robust death registration systems, the cause of death (COD) is determined through a verbal autopsy (VA) report. A non-clinician field agent completes a VA report using a set of standardized questions as guide to identify the symptoms of a COD. The narrative text of the VA report is used as a case study to examine the difficulties of applying NLP techniques to the healthcare domain. This paper presents a framework that leverages knowledge across multiple domains via two domain adaptation techniques: feature extraction and fine-tuning. These techniques aim to improve VA text representations for COD classification tasks in the health domain. The framework is motivated by multi-step learning, where a final learning task is realized via a sequence of intermediate learning tasks. The framework builds upon the strengths of the Bidirectional Encoder Representations from Transformers (BERT) and Embeddings from Language Models (ELMo) models pretrained on the general English and biomedical domains. These models are employed to extract features from the VA narratives. Our results demonstrate improved performance when initializing the learning of BERT embeddings with ELMo embeddings. The benefit of incorporating character-level information for learning word embeddings in the English domain, coupled with word-level information for learning word embeddings in the biomedical domain, is also evident.},
author = {Manaka, Thokozile and Zyl, Terence Van and Kar, Deepak and Wade, Alisha},
doi = {10.1007/s11063-024-11526-y},
file = {:C\:/Users/USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Manaka et al. - 2024 - Multi-step Transfer Learning in Natural Language Processing for the Health Domain.pdf:pdf},
issn = {1573773X},
journal = {Neural Processing Letters},
keywords = {Feature extraction,Fine tuning,Natural language processing,Text classification,Transfer learning,Verbal autopsy},
month = {jun},
number = {3},
pages = {1--26},
publisher = {Springer},
title = {{Multi-step Transfer Learning in Natural Language Processing for the Health Domain}},
url = {https://link-springer-com.ucu.sempertool.dk/article/10.1007/s11063-024-11526-y},
volume = {56},
year = {2024}
}
@article{Dhyani2021,
abstract = {ABSTRACT Transfer learning is a discipline that is expanding quickly within the realm of natural language processing (NLP) and machine learning. It is the application of previously learned models to the solution of a variety of problems that are connected to one another. This paper presents a comprehensive survey of transfer learning techniques in NLP, focusing on five key classification algorithms: (1) BERT, (2) GPT, (3) ELMo, (4) RoBERTa, and (5) ALBERT. We discuss the fundamental concepts, methodologies, and performance benchmarks of each algorithm, highlighting the various approaches taken to leverage pre-existing knowledge for effective learning. Furthermore, we provide an overview of the latest advancements and challenges in transfer learning for NLP, along with promising directions for future research in this domain.},
archivePrefix = {arXiv},
arxivId = {2007.04239},
author = {Dhyani, Bijesh},
doi = {10.17762/msea.v70i1.2312},
eprint = {2007.04239},
file = {:C\:/Users/USER/AppData/Local/Mendeley Ltd./Mendeley Desktop/Downloaded/Alyafeai, Saeed AlShaibani, Ahmad - 2020 - A SURVEY ON TRANSFER LEARNING IN NATURAL LANGUAGE PROCESSING A PREPRINT.pdf:pdf},
issn = {2094-0343},
journal = {Mathematical Statistician and Engineering Applications},
keywords = {Learning {\textperiodcentered} NLP {\textperiodcentered},Survey,Transfer},
month = {jan},
number = {1},
pages = {303--311},
title = {{Transfer Learning in Natural Language Processing: A Survey}},
url = {http://arxiv.org/abs/2007.04239 https://www.philstat.org/index.php/MSEA/article/view/2312},
volume = {70},
year = {2021}
}
